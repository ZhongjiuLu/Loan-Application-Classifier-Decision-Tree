---
title: "Loan Application Classifier"
author: "Zhongjiu Lu"
output: pdf_document
---
```{r}
install.packages("C50")
library(C50)
library(gmodels)
library(caret)
```

1. Import the pre-processed data set in R. Shuffle the records and split them into a training set (20,000 records), a validation set (8,000 records) and a test set (all remaining records).

```{r}
#load data
df <- read.csv("./Loans_processed.csv",header=TRUE)
#Split the data set
set.seed(3)
n <- nrow(df) # number of observations n
shuffled_df <- df[sample(n), ] # shuffled the datasets
train <- shuffled_df[1:20000, ] # training set (20,000 records)
validation <- shuffled_df[20001:28000, ] # validation set (8,000 records)
test <- shuffled_df[28001:n, ] # a test set (all remaining records)
```

2. Build classification tree
```{r}
#train model
train_x <- train[,1:7] #training set of predictors
train_y <- train[,'loan_status'] # training set of response

validation_x <- validation[,1:7] # validation set of predictors
validation_y <- validation[,"loan_status"] # validation set of response
  
model1 <- C5.0(x = train_x, y = train_y) # taining model1
summary(model1)
pred <- predict(model1, validation_x) # prediction result on validation
summary(pred) # result of prediction
# CrossTable(validation$loan_status, pred,
#              prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
#              dnn = c('actual', 'predicted'))
table(validation_y, pred, dnn = c('actual', 'predicted')) 
# confusion matrix of performance on the validation set

pred <- predict(model1, train_x) # prediction result on training
table(train_y, pred, dnn = c('actual', 'predicted')) 
# confusion matrix of performance on the training set

```

As we can see, the model essentially predict all data to "Fully Paid". Therefore, the accuracy is equal to $\frac{\#\ of\ repaid\ loans}{\#\ of\ repaid\ loans\ +\ \#\ of\ charged\ off\ loans}$ and cannot be greater than the number for both training and validation sets.

3. Model with cost matrix
```{r}
#model with around 50% sensitivity
error_cost <- matrix(c(0, 4.85, 1, 0), nrow = 2)
model2 <- C5.0(x=train_x, y=train_y, costs = error_cost)
pred2 <- predict(model2, validation_x)
sensitivity(pred2, validation_y)
#report precision
precision(pred2, validation$loan_status)
```
Columns corresponds to the true classes and rows are the predicted classes in C50 package. In terms of business importance, the cost of predicting on people who actually paid off their loan with actual status is charged off is much higher (false positive) than predicting people with charged off, but actually it is paid off (false negative). Therefore, we should put more weight on false positive than false negative.

as most of the fraudulent transactions are identified, probably at loss of precision, since it is very important that all fraud is identified or at least suspicions are raised.
```{r}

#model with around 40% sensitivity
error_cost <- matrix(c(0, 9, 2, 0), nrow = 2)
model3 <- C5.0(x=train[,-8],y=train[,8], costs = error_cost)
pred3 <- predict(model3, validation[,-8])
sensitivity(pred3, validation$loan_status)
#report precision
precision(pred3, validation$loan_status)

#model with around 25% sensitivity
error_cost <- matrix(c(0, 10, 3, 0), nrow = 2)
model4 <- C5.0(x=train[,-8],y=train[,8], costs = error_cost)
pred4 <- predict(model4, validation[,-8])
sensitivity(pred4, validation$loan_status)
#report precision
precision(pred4, validation$loan_status)
```

4.Evaluate the performance of your cost parameter matrix on the test set.

```{r}
cost_parameter <- matrix(c(0, 9, 2, 0), nrow = 2)# assume this is the selected cost parameter matrix
final_model <- C5.0(x=train_x,y=train_y, costs = cost_parameter)
final_pred <- predict(final_model, test[,-8])
#report precision

table(test[,8], final_pred, dnn = c('actual', 'predicted')) 
```

